# PLANNING ETAPA 1 - Implementa√ß√£o de C√≥digo

> Guia para implementa√ß√£o no Cursor - An√°lise Descritiva da Exposi√ß√£o √† IA Generativa no Brasil

---

## ESTRUTURA DO PROJETO

```
etapa1_ia_generativa/
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.py              # Configura√ß√µes e constantes do projeto
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                     # Dados brutos (n√£o versionados)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ilo_scores.xlsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pnad_2025q3.parquet
‚îÇ   ‚îî‚îÄ‚îÄ processed/               # Dados processados
‚îÇ       ‚îú‚îÄ‚îÄ ilo_exposure_clean.csv
‚îÇ       ‚îú‚îÄ‚îÄ pnad_clean.csv
‚îÇ       ‚îî‚îÄ‚îÄ pnad_ilo_merged.csv
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ 01_download_pnad.py      # Download dados PNAD via BigQuery
‚îÇ   ‚îú‚îÄ‚îÄ 02_process_ilo.py        # Processar planilha ILO
‚îÇ   ‚îú‚îÄ‚îÄ 03_clean_pnad.py         # Limpeza e vari√°veis derivadas
‚îÇ   ‚îú‚îÄ‚îÄ 04_crosswalk.py          # Correspond√™ncia COD-ISCO
‚îÇ   ‚îú‚îÄ‚îÄ 05_merge_data.py         # Merge final PNAD + ILO
‚îÇ   ‚îú‚îÄ‚îÄ 06_analysis_tables.py    # Gera√ß√£o das 5 tabelas
‚îÇ   ‚îú‚îÄ‚îÄ 07_analysis_figures.py   # Gera√ß√£o das 4 figuras
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ weighted_stats.py    # Fun√ß√µes de estat√≠sticas ponderadas
‚îÇ       ‚îî‚îÄ‚îÄ validators.py        # Fun√ß√µes de valida√ß√£o
‚îÇ
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ tables/                  # Tabelas em CSV e LaTeX
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tabela1_exposicao_grupos.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tabela1_exposicao_grupos.tex
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tabela2_perfil_quintis.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tabela2_perfil_quintis.tex
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tabela3_regiao_setor.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tabela3_regiao_setor.tex
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tabela4_desigualdade.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tabela4_desigualdade.tex
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tabela5_comparacao.csv
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ figures/                 # Gr√°ficos em PNG e PDF
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fig1_distribuicao_exposicao.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fig1_distribuicao_exposicao.pdf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fig2_heatmap_regiao_setor.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fig2_heatmap_regiao_setor.pdf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fig3_renda_exposicao.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fig3_renda_exposicao.pdf
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fig4_decomposicao_demografica.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fig4_decomposicao_demografica.pdf
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ logs/                    # Logs de execu√ß√£o
‚îÇ       ‚îú‚îÄ‚îÄ 01_download.log
‚îÇ       ‚îú‚îÄ‚îÄ 02_ilo_process.log
‚îÇ       ‚îú‚îÄ‚îÄ 03_pnad_clean.log
‚îÇ       ‚îú‚îÄ‚îÄ 04_crosswalk.log
‚îÇ       ‚îú‚îÄ‚îÄ 05_merge.log
‚îÇ       ‚îú‚îÄ‚îÄ 06_tables.log
‚îÇ       ‚îî‚îÄ‚îÄ 07_figures.log
‚îÇ
‚îú‚îÄ‚îÄ tests/                       # Testes de valida√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ test_01_download.py
‚îÇ   ‚îú‚îÄ‚îÄ test_02_ilo.py
‚îÇ   ‚îú‚îÄ‚îÄ test_03_clean.py
‚îÇ   ‚îú‚îÄ‚îÄ test_04_crosswalk.py
‚îÇ   ‚îî‚îÄ‚îÄ test_05_merge.py
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                   # Notebooks explorat√≥rios (opcional)
‚îÇ   ‚îî‚îÄ‚îÄ exploratory_analysis.ipynb
‚îÇ
‚îú‚îÄ‚îÄ RESULTADOS_ETAPA1.md         # Relat√≥rio final com an√°lise
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ run_all.py                   # Script master para rodar tudo
```

---

## PARTE 1: CONFIGURA√á√ÉO GOOGLE CLOUD

### 1.1 Criar Projeto no Google Cloud Console

```
1. Acesse: https://console.cloud.google.com/
2. Crie um novo projeto (ex: "dissertacao-ia-br")
3. Anote o PROJECT_ID (vai usar nos scripts)
```

### 1.2 Ativar APIs Necess√°rias

```
No Cloud Console, ative:
- BigQuery API
- Cloud Storage API (opcional, para cache)
```

### 1.3 Configurar Credenciais

```bash
# Instalar Google Cloud CLI
# Mac: brew install google-cloud-sdk
# Windows: https://cloud.google.com/sdk/docs/install

# Autenticar
gcloud auth login
gcloud auth application-default login

# Definir projeto padr√£o
gcloud config set project SEU_PROJECT_ID
```

### 1.4 Configurar Billing

```
1. No Cloud Console > Billing
2. Vincule uma conta de faturamento ao projeto
3. IMPORTANTE: BigQuery tem 1TB/m√™s gr√°tis de queries
4. A query da PNAD usa ~500MB, ent√£o est√° no free tier
```

### 1.5 Testar Conex√£o

```python
# Executar no Python para validar
import basedosdados as bd

# Teste simples
test = bd.read_sql(
    "SELECT COUNT(*) as n FROM `basedosdados.br_ibge_pnad_continua.microdados` WHERE ano = 2024 LIMIT 1",
    billing_project_id="SEU_PROJECT_ID"
)
print(test)
```

---

## PARTE 2: IMPLEMENTA√á√ÉO DOS SCRIPTS

### 2.0 config/settings.py

```python
"""
Configura√ß√µes globais do projeto Etapa 1
"""
import os
from pathlib import Path

# Diret√≥rios
ROOT_DIR = Path(__file__).parent.parent
DATA_RAW = ROOT_DIR / "data" / "raw"
DATA_PROCESSED = ROOT_DIR / "data" / "processed"
OUTPUTS_TABLES = ROOT_DIR / "outputs" / "tables"
OUTPUTS_FIGURES = ROOT_DIR / "outputs" / "figures"
OUTPUTS_LOGS = ROOT_DIR / "outputs" / "logs"

# Criar diret√≥rios se n√£o existirem
for dir_path in [DATA_RAW, DATA_PROCESSED, OUTPUTS_TABLES, OUTPUTS_FIGURES, OUTPUTS_LOGS]:
    dir_path.mkdir(parents=True, exist_ok=True)

# Google Cloud
GCP_PROJECT_ID = "SEU_PROJECT_ID"  # <-- ALTERAR AQUI

# PNAD
PNAD_ANO = 2025
PNAD_TRIMESTRE = 3

# ILO
ILO_FILE = DATA_RAW / "Final_Scores_ISCO08_Gmyrek_et_al_2025.xlsx"
ILO_URL = "https://github.com/pgmyrek/2025_GenAI_scores_ISCO08/raw/main/Final_Scores_ISCO08_Gmyrek_et_al_2025.xlsx"

# Mapeamentos
REGIAO_MAP = {
    'RO': 'Norte', 'AC': 'Norte', 'AM': 'Norte', 'RR': 'Norte', 
    'PA': 'Norte', 'AP': 'Norte', 'TO': 'Norte',
    'MA': 'Nordeste', 'PI': 'Nordeste', 'CE': 'Nordeste', 'RN': 'Nordeste',
    'PB': 'Nordeste', 'PE': 'Nordeste', 'AL': 'Nordeste', 'SE': 'Nordeste', 'BA': 'Nordeste',
    'MG': 'Sudeste', 'ES': 'Sudeste', 'RJ': 'Sudeste', 'SP': 'Sudeste',
    'PR': 'Sul', 'SC': 'Sul', 'RS': 'Sul',
    'MS': 'Centro-Oeste', 'MT': 'Centro-Oeste', 'GO': 'Centro-Oeste', 'DF': 'Centro-Oeste'
}

GRANDES_GRUPOS = {
    '1': 'Dirigentes e gerentes',
    '2': 'Profissionais das ci√™ncias',
    '3': 'T√©cnicos n√≠vel m√©dio',
    '4': 'Apoio administrativo',
    '5': 'Servi√ßos e vendedores',
    '6': 'Agropecu√°ria qualificada',
    '7': 'Ind√∫stria qualificada',
    '8': 'Operadores de m√°quinas',
    '9': 'Ocupa√ß√µes elementares'
}

RACA_MAP = {
    'Branca': 'Branca',
    'Preta': 'Negra',
    'Parda': 'Negra',
    'Amarela': 'Outras',
    'Ind√≠gena': 'Outras'
}

# Faixas et√°rias
IDADE_BINS = [0, 25, 35, 45, 55, 100]
IDADE_LABELS = ['18-24', '25-34', '35-44', '45-54', '55+']
```

---

### 2.1 src/01_download_pnad.py

```python
"""
Script 01: Download dos microdados PNAD via BigQuery
Entrada: Query BigQuery
Sa√≠da: data/raw/pnad_2025q3.parquet
"""

import logging
import basedosdados as bd
import pandas as pd
import sys
sys.path.append('..')
from config.settings import *

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(OUTPUTS_LOGS / '01_download.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def download_pnad():
    """Baixa microdados PNAD do BigQuery"""
    
    logger.info(f"Iniciando download PNAD {PNAD_ANO} Q{PNAD_TRIMESTRE}")
    
    query = f"""
    SELECT 
        ano,
        trimestre,
        sigla_uf,
        v2007 AS sexo,
        v2009 AS idade,
        v2010 AS raca_cor,
        vd3004 AS nivel_instrucao,
        v4010 AS cod_ocupacao,
        v4013 AS grupamento_atividade,
        vd4009 AS posicao_ocupacao,
        vd4020 AS rendimento_habitual,
        vd4016 AS rendimento_todos,
        v4019 AS horas_trabalhadas,
        v1028 AS peso
    FROM `basedosdados.br_ibge_pnad_continua.microdados`
    WHERE ano = {PNAD_ANO} 
        AND trimestre = {PNAD_TRIMESTRE}
        AND v4010 IS NOT NULL
        AND vd4020 > 0
    """
    
    logger.info("Executando query no BigQuery...")
    df = bd.read_sql(query, billing_project_id=GCP_PROJECT_ID)
    
    logger.info(f"Download conclu√≠do: {len(df):,} observa√ß√µes")
    logger.info(f"Colunas: {list(df.columns)}")
    
    # Salvar
    output_path = DATA_RAW / f"pnad_{PNAD_ANO}q{PNAD_TRIMESTRE}.parquet"
    df.to_parquet(output_path, index=False)
    logger.info(f"Salvo em: {output_path}")
    
    # Estat√≠sticas b√°sicas
    logger.info(f"Popula√ß√£o representada: {df['peso'].sum()/1e6:.1f} milh√µes")
    logger.info(f"UFs presentes: {df['sigla_uf'].nunique()}")
    
    return df

if __name__ == "__main__":
    download_pnad()
```

---

### 2.2 src/02_process_ilo.py

```python
"""
Script 02: Processar planilha ILO com scores de exposi√ß√£o
Entrada: data/raw/Final_Scores_ISCO08_Gmyrek_et_al_2025.xlsx
Sa√≠da: data/processed/ilo_exposure_clean.csv
"""

import logging
import pandas as pd
import requests
import sys
sys.path.append('..')
from config.settings import *

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(OUTPUTS_LOGS / '02_ilo_process.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def download_ilo_if_needed():
    """Baixa arquivo ILO do GitHub se n√£o existir"""
    if not ILO_FILE.exists():
        logger.info(f"Baixando arquivo ILO de: {ILO_URL}")
        response = requests.get(ILO_URL)
        with open(ILO_FILE, 'wb') as f:
            f.write(response.content)
        logger.info("Download conclu√≠do")

def process_ilo():
    """Processa planilha ILO e extrai scores por ocupa√ß√£o"""
    
    download_ilo_if_needed()
    
    logger.info(f"Lendo arquivo: {ILO_FILE}")
    df_raw = pd.read_excel(ILO_FILE)
    
    logger.info(f"Linhas raw (tarefas): {len(df_raw):,}")
    logger.info(f"Colunas dispon√≠veis: {list(df_raw.columns)}")
    
    # Identificar colunas corretas (podem variar ligeiramente)
    # Ajustar conforme estrutura real do arquivo
    col_mapping = {
        'ISCO_08': 'isco_08',
        'Title': 'occupation_title',
        'mean_score_2025': 'exposure_score',
        'SD_2025': 'exposure_sd',
        'potential25': 'exposure_gradient'
    }
    
    # Verificar se colunas existem
    available_cols = [c for c in col_mapping.keys() if c in df_raw.columns]
    logger.info(f"Colunas encontradas: {available_cols}")
    
    # Renomear
    df = df_raw.rename(columns={k: v for k, v in col_mapping.items() if k in df_raw.columns})
    
    # Agregar por ocupa√ß√£o (arquivo tem m√∫ltiplas tarefas por ocupa√ß√£o)
    df_agg = df.groupby('isco_08').agg({
        'occupation_title': 'first',
        'exposure_score': 'mean',
        'exposure_sd': 'mean',
        'exposure_gradient': 'first'
    }).reset_index()
    
    logger.info(f"Ocupa√ß√µes √∫nicas: {len(df_agg):,}")
    logger.info(f"Score m√©dio: {df_agg['exposure_score'].mean():.3f}")
    logger.info(f"Score range: [{df_agg['exposure_score'].min():.3f}, {df_agg['exposure_score'].max():.3f}]")
    
    # Garantir formato string com 4 d√≠gitos
    df_agg['isco_08_str'] = df_agg['isco_08'].astype(str).str.zfill(4)
    
    # Distribui√ß√£o por gradiente
    logger.info("\nDistribui√ß√£o por gradiente:")
    for grad, count in df_agg['exposure_gradient'].value_counts().items():
        logger.info(f"  {grad}: {count} ocupa√ß√µes")
    
    # Salvar
    output_path = DATA_PROCESSED / "ilo_exposure_clean.csv"
    df_agg.to_csv(output_path, index=False)
    logger.info(f"\nSalvo em: {output_path}")
    
    return df_agg

if __name__ == "__main__":
    process_ilo()
```

---

### 2.3 src/03_clean_pnad.py

```python
"""
Script 03: Limpeza e cria√ß√£o de vari√°veis derivadas PNAD
Entrada: data/raw/pnad_2025q3.parquet
Sa√≠da: data/processed/pnad_clean.csv
"""

import logging
import pandas as pd
import numpy as np
import sys
sys.path.append('..')
from config.settings import *

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(OUTPUTS_LOGS / '03_pnad_clean.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def clean_pnad():
    """Limpa e prepara dados PNAD"""
    
    input_path = DATA_RAW / f"pnad_{PNAD_ANO}q{PNAD_TRIMESTRE}.parquet"
    logger.info(f"Lendo: {input_path}")
    df = pd.read_parquet(input_path)
    
    n_inicial = len(df)
    logger.info(f"Observa√ß√µes iniciais: {n_inicial:,}")
    
    # --- LIMPEZA ---
    
    # 1. Converter tipos
    df['cod_ocupacao'] = df['cod_ocupacao'].astype(str).str.zfill(4)
    df['idade'] = pd.to_numeric(df['idade'], errors='coerce')
    df['rendimento_habitual'] = pd.to_numeric(df['rendimento_habitual'], errors='coerce')
    df['peso'] = pd.to_numeric(df['peso'], errors='coerce')
    
    # 2. Remover missings cr√≠ticos
    df = df.dropna(subset=['cod_ocupacao', 'idade', 'peso'])
    logger.info(f"Ap√≥s remover missings cr√≠ticos: {len(df):,} ({len(df)/n_inicial:.1%})")
    
    # 3. Filtrar faixa et√°ria (18-65)
    df = df[(df['idade'] >= 18) & (df['idade'] <= 65)]
    logger.info(f"Ap√≥s filtrar 18-65 anos: {len(df):,} ({len(df)/n_inicial:.1%})")
    
    # 4. Remover ocupa√ß√µes inv√°lidas (c√≥digo 0000 ou similar)
    df = df[~df['cod_ocupacao'].isin(['0000', '9999'])]
    logger.info(f"Ap√≥s remover ocupa√ß√µes inv√°lidas: {len(df):,}")
    
    # --- VARI√ÅVEIS DERIVADAS ---
    
    # Formalidade
    df['formal'] = df['posicao_ocupacao'].isin(['01', '03', '05']).astype(int)
    logger.info(f"Taxa de formalidade: {df['formal'].mean():.1%}")
    
    # Faixas et√°rias
    df['faixa_etaria'] = pd.cut(
        df['idade'],
        bins=IDADE_BINS,
        labels=IDADE_LABELS
    )
    
    # Regi√£o
    df['regiao'] = df['sigla_uf'].map(REGIAO_MAP)
    
    # Ra√ßa agregada
    df['raca_agregada'] = df['raca_cor'].map(RACA_MAP)
    
    # Grande grupo ocupacional
    df['grande_grupo'] = df['cod_ocupacao'].str[0].map(GRANDES_GRUPOS)
    
    # Sexo como texto
    df['sexo_texto'] = df['sexo'].map({1: 'Homem', 2: 'Mulher', '1': 'Homem', '2': 'Mulher'})
    
    # Winsoriza√ß√£o de renda (percentil 99)
    p99 = df['rendimento_habitual'].quantile(0.99)
    df['rendimento_winsor'] = df['rendimento_habitual'].clip(upper=p99)
    logger.info(f"Percentil 99 renda: R$ {p99:,.0f}")
    
    # --- VALIDA√á√ïES ---
    
    logger.info("\n=== VALIDA√á√ïES ===")
    logger.info(f"Ocupa√ß√µes √∫nicas (COD): {df['cod_ocupacao'].nunique()}")
    logger.info(f"UFs: {df['sigla_uf'].nunique()}")
    logger.info(f"Regi√µes: {df['regiao'].nunique()}")
    logger.info(f"Popula√ß√£o representada: {df['peso'].sum()/1e6:.1f} milh√µes")
    
    logger.info("\nDistribui√ß√£o por sexo:")
    for sexo, peso in df.groupby('sexo_texto')['peso'].sum().items():
        logger.info(f"  {sexo}: {peso/1e6:.1f} milh√µes")
    
    logger.info("\nDistribui√ß√£o por regi√£o:")
    for regiao, peso in df.groupby('regiao')['peso'].sum().sort_values(ascending=False).items():
        logger.info(f"  {regiao}: {peso/1e6:.1f} milh√µes")
    
    # Salvar
    output_path = DATA_PROCESSED / "pnad_clean.csv"
    df.to_csv(output_path, index=False)
    logger.info(f"\nSalvo em: {output_path}")
    
    return df

if __name__ == "__main__":
    clean_pnad()
```

---

### 2.4 src/04_crosswalk.py

```python
"""
Script 04: Crosswalk hier√°rquico COD ‚Üí ISCO-08
Entrada: data/processed/pnad_clean.csv, data/processed/ilo_exposure_clean.csv
Sa√≠da: Log com estat√≠sticas de match por n√≠vel
"""

import logging
import pandas as pd
import numpy as np
import sys
sys.path.append('..')
from config.settings import *

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(OUTPUTS_LOGS / '04_crosswalk.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def create_ilo_aggregations(df_ilo):
    """Cria agrega√ß√µes ILO em diferentes n√≠veis hier√°rquicos"""
    
    ilo_4d = df_ilo.groupby('isco_08_str')['exposure_score'].mean().to_dict()
    ilo_3d = df_ilo.groupby(df_ilo['isco_08_str'].str[:3])['exposure_score'].mean().to_dict()
    ilo_2d = df_ilo.groupby(df_ilo['isco_08_str'].str[:2])['exposure_score'].mean().to_dict()
    ilo_1d = df_ilo.groupby(df_ilo['isco_08_str'].str[:1])['exposure_score'].mean().to_dict()
    
    logger.info(f"C√≥digos ILO 4-digit: {len(ilo_4d)}")
    logger.info(f"C√≥digos ILO 3-digit: {len(ilo_3d)}")
    logger.info(f"C√≥digos ILO 2-digit: {len(ilo_2d)}")
    logger.info(f"C√≥digos ILO 1-digit: {len(ilo_1d)}")
    
    return ilo_4d, ilo_3d, ilo_2d, ilo_1d

def hierarchical_crosswalk(df_pnad, ilo_4d, ilo_3d, ilo_2d, ilo_1d):
    """Aplica crosswalk hier√°rquico em 4 n√≠veis"""
    
    logger.info("\n=== CROSSWALK HIER√ÅRQUICO ===")
    
    # Inicializar colunas
    df_pnad['exposure_score'] = np.nan
    df_pnad['match_level'] = None
    
    # N√≠vel 1: 4-digit
    mask_4d = df_pnad['cod_ocupacao'].isin(ilo_4d.keys())
    df_pnad.loc[mask_4d, 'exposure_score'] = df_pnad.loc[mask_4d, 'cod_ocupacao'].map(ilo_4d)
    df_pnad.loc[mask_4d, 'match_level'] = '4-digit'
    logger.info(f"Match 4-digit: {mask_4d.sum():,} ({mask_4d.mean():.1%})")
    
    # N√≠vel 2: 3-digit
    mask_missing = df_pnad['exposure_score'].isna()
    cod_3d = df_pnad.loc[mask_missing, 'cod_ocupacao'].str[:3]
    mask_3d = cod_3d.isin(ilo_3d.keys())
    idx_3d = mask_missing[mask_missing].index[mask_3d.values]
    df_pnad.loc[idx_3d, 'exposure_score'] = cod_3d[mask_3d].map(ilo_3d).values
    df_pnad.loc[idx_3d, 'match_level'] = '3-digit'
    logger.info(f"Match 3-digit: {len(idx_3d):,} ({len(idx_3d)/len(df_pnad):.1%})")
    
    # N√≠vel 3: 2-digit
    mask_missing = df_pnad['exposure_score'].isna()
    cod_2d = df_pnad.loc[mask_missing, 'cod_ocupacao'].str[:2]
    mask_2d = cod_2d.isin(ilo_2d.keys())
    idx_2d = mask_missing[mask_missing].index[mask_2d.values]
    df_pnad.loc[idx_2d, 'exposure_score'] = cod_2d[mask_2d].map(ilo_2d).values
    df_pnad.loc[idx_2d, 'match_level'] = '2-digit'
    logger.info(f"Match 2-digit: {len(idx_2d):,} ({len(idx_2d)/len(df_pnad):.1%})")
    
    # N√≠vel 4: 1-digit
    mask_missing = df_pnad['exposure_score'].isna()
    cod_1d = df_pnad.loc[mask_missing, 'cod_ocupacao'].str[:1]
    mask_1d = cod_1d.isin(ilo_1d.keys())
    idx_1d = mask_missing[mask_missing].index[mask_1d.values]
    df_pnad.loc[idx_1d, 'exposure_score'] = cod_1d[mask_1d].map(ilo_1d).values
    df_pnad.loc[idx_1d, 'match_level'] = '1-digit'
    logger.info(f"Match 1-digit: {len(idx_1d):,} ({len(idx_1d)/len(df_pnad):.1%})")
    
    # Sem match
    mask_no_match = df_pnad['exposure_score'].isna()
    logger.info(f"Sem match: {mask_no_match.sum():,} ({mask_no_match.mean():.1%})")
    
    return df_pnad

def validate_crosswalk(df):
    """Valida resultados do crosswalk"""
    
    logger.info("\n=== VALIDA√á√ÉO DO CROSSWALK ===")
    
    # Cobertura total
    coverage = df['exposure_score'].notna().mean()
    logger.info(f"Cobertura total: {coverage:.1%}")
    
    # Distribui√ß√£o por n√≠vel
    logger.info("\nDistribui√ß√£o por n√≠vel de match:")
    for level, count in df['match_level'].value_counts().items():
        pct = count / len(df) * 100
        logger.info(f"  {level}: {count:,} ({pct:.1f}%)")
    
    # Estat√≠sticas de score
    logger.info("\nEstat√≠sticas do score de exposi√ß√£o:")
    logger.info(f"  M√©dia: {df['exposure_score'].mean():.3f}")
    logger.info(f"  Desvio-padr√£o: {df['exposure_score'].std():.3f}")
    logger.info(f"  M√≠nimo: {df['exposure_score'].min():.3f}")
    logger.info(f"  M√°ximo: {df['exposure_score'].max():.3f}")
    
    # Sanity check: exposi√ß√£o por grande grupo
    logger.info("\nExposi√ß√£o m√©dia por grande grupo (sanity check):")
    exp_grupos = df.groupby('grande_grupo').apply(
        lambda x: np.average(x['exposure_score'].dropna(), weights=x.loc[x['exposure_score'].notna(), 'peso'])
    ).sort_values(ascending=False)
    for grupo, score in exp_grupos.items():
        logger.info(f"  {grupo}: {score:.3f}")
    
    return coverage

def run_crosswalk():
    """Executa crosswalk completo"""
    
    # Carregar dados
    df_pnad = pd.read_csv(DATA_PROCESSED / "pnad_clean.csv")
    df_ilo = pd.read_csv(DATA_PROCESSED / "ilo_exposure_clean.csv")
    
    logger.info(f"PNAD: {len(df_pnad):,} observa√ß√µes")
    logger.info(f"ILO: {len(df_ilo):,} ocupa√ß√µes")
    
    # Criar agrega√ß√µes
    ilo_4d, ilo_3d, ilo_2d, ilo_1d = create_ilo_aggregations(df_ilo)
    
    # Aplicar crosswalk
    df_result = hierarchical_crosswalk(df_pnad.copy(), ilo_4d, ilo_3d, ilo_2d, ilo_1d)
    
    # Validar
    coverage = validate_crosswalk(df_result)
    
    # Retornar para uso no pr√≥ximo script
    return df_result, coverage

if __name__ == "__main__":
    df, coverage = run_crosswalk()
    print(f"\n‚úì Crosswalk conclu√≠do com {coverage:.1%} de cobertura")
```

---

### 2.5 src/05_merge_data.py

```python
"""
Script 05: Merge final e cria√ß√£o da base consolidada
Entrada: Dados do crosswalk
Sa√≠da: data/processed/pnad_ilo_merged.csv
"""

import logging
import pandas as pd
import numpy as np
import sys
sys.path.append('..')
from config.settings import *
from src.s04_crosswalk import run_crosswalk

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(OUTPUTS_LOGS / '05_merge.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def add_gradient_classification(df):
    """Adiciona classifica√ß√£o por gradiente ILO"""
    
    # Definir thresholds baseados na metodologia ILO
    def classify_gradient(score):
        if pd.isna(score):
            return 'Sem classifica√ß√£o'
        elif score < 0.22:
            return 'Not Exposed'
        elif score < 0.36:
            return 'Minimal Exposure'
        elif score < 0.45:
            return 'Gradient 1-2'
        elif score < 0.55:
            return 'Gradient 3'
        else:
            return 'Gradient 4 (Alta)'
    
    df['exposure_gradient'] = df['exposure_score'].apply(classify_gradient)
    
    logger.info("\nDistribui√ß√£o por gradiente:")
    for grad, peso in df.groupby('exposure_gradient')['peso'].sum().sort_values(ascending=False).items():
        logger.info(f"  {grad}: {peso/1e6:.1f} milh√µes")
    
    return df

def add_quintiles(df):
    """Adiciona quintis e decis de exposi√ß√£o"""
    
    # Filtrar apenas com score v√°lido
    mask = df['exposure_score'].notna()
    
    # Quintis
    df.loc[mask, 'quintil_exposure'] = pd.qcut(
        df.loc[mask, 'exposure_score'],
        q=5,
        labels=['Q1 (Baixa)', 'Q2', 'Q3', 'Q4', 'Q5 (Alta)'],
        duplicates='drop'
    )
    
    # Decis
    df.loc[mask, 'decil_exposure'] = pd.qcut(
        df.loc[mask, 'exposure_score'],
        q=10,
        labels=[f'D{i}' for i in range(1, 11)],
        duplicates='drop'
    )
    
    return df

def create_sector_aggregation(df):
    """Cria agrega√ß√£o setorial simplificada"""
    
    # Mapear CNAE para setores agregados (ajustar conforme c√≥digos reais)
    setor_map = {
        # Agricultura
        '01': 'Agropecu√°ria', '02': 'Agropecu√°ria', '03': 'Agropecu√°ria',
        # Ind√∫stria
        '05': 'Ind√∫stria', '06': 'Ind√∫stria', '07': 'Ind√∫stria', '08': 'Ind√∫stria',
        '10': 'Ind√∫stria', '11': 'Ind√∫stria', '12': 'Ind√∫stria', '13': 'Ind√∫stria',
        '14': 'Ind√∫stria', '15': 'Ind√∫stria', '16': 'Ind√∫stria', '17': 'Ind√∫stria',
        '18': 'Ind√∫stria', '19': 'Ind√∫stria', '20': 'Ind√∫stria', '21': 'Ind√∫stria',
        '22': 'Ind√∫stria', '23': 'Ind√∫stria', '24': 'Ind√∫stria', '25': 'Ind√∫stria',
        '26': 'Ind√∫stria', '27': 'Ind√∫stria', '28': 'Ind√∫stria', '29': 'Ind√∫stria',
        '30': 'Ind√∫stria', '31': 'Ind√∫stria', '32': 'Ind√∫stria', '33': 'Ind√∫stria',
        # Constru√ß√£o
        '41': 'Constru√ß√£o', '42': 'Constru√ß√£o', '43': 'Constru√ß√£o',
        # Com√©rcio
        '45': 'Com√©rcio', '46': 'Com√©rcio', '47': 'Com√©rcio',
        # Servi√ßos
        '49': 'Servi√ßos', '50': 'Servi√ßos', '51': 'Servi√ßos', '52': 'Servi√ßos',
        '53': 'Servi√ßos', '55': 'Servi√ßos', '56': 'Servi√ßos',
        # TIC / Servi√ßos especializados
        '58': 'TIC e Servi√ßos Prof.', '59': 'TIC e Servi√ßos Prof.', '60': 'TIC e Servi√ßos Prof.',
        '61': 'TIC e Servi√ßos Prof.', '62': 'TIC e Servi√ßos Prof.', '63': 'TIC e Servi√ßos Prof.',
        '64': 'TIC e Servi√ßos Prof.', '65': 'TIC e Servi√ßos Prof.', '66': 'TIC e Servi√ßos Prof.',
        '69': 'TIC e Servi√ßos Prof.', '70': 'TIC e Servi√ßos Prof.', '71': 'TIC e Servi√ßos Prof.',
        '72': 'TIC e Servi√ßos Prof.', '73': 'TIC e Servi√ßos Prof.', '74': 'TIC e Servi√ßos Prof.',
        '75': 'TIC e Servi√ßos Prof.',
        # Administra√ß√£o p√∫blica
        '84': 'Administra√ß√£o P√∫blica',
        # Educa√ß√£o
        '85': 'Educa√ß√£o',
        # Sa√∫de
        '86': 'Sa√∫de', '87': 'Sa√∫de', '88': 'Sa√∫de',
    }
    
    # Extrair 2 primeiros d√≠gitos do grupamento de atividade
    if df['grupamento_atividade'].dtype == 'object':
        df['cnae_2d'] = df['grupamento_atividade'].astype(str).str[:2]
    else:
        df['cnae_2d'] = df['grupamento_atividade'].astype(str).str[:2]
    
    df['setor_agregado'] = df['cnae_2d'].map(setor_map).fillna('Outros Servi√ßos')
    
    return df

def merge_and_finalize():
    """Executa merge final e salva base consolidada"""
    
    logger.info("=== MERGE FINAL ===\n")
    
    # Executar crosswalk
    df, coverage = run_crosswalk()
    
    # Adicionar classifica√ß√µes
    df = add_gradient_classification(df)
    df = add_quintiles(df)
    df = create_sector_aggregation(df)
    
    # Estat√≠sticas finais
    logger.info("\n=== BASE FINAL ===")
    logger.info(f"Total de observa√ß√µes: {len(df):,}")
    logger.info(f"Com score de exposi√ß√£o: {df['exposure_score'].notna().sum():,}")
    logger.info(f"Cobertura: {df['exposure_score'].notna().mean():.1%}")
    logger.info(f"Popula√ß√£o representada: {df['peso'].sum()/1e6:.1f} milh√µes")
    
    # Colunas finais
    cols_output = [
        'ano', 'trimestre', 'sigla_uf', 'regiao',
        'sexo', 'sexo_texto', 'idade', 'faixa_etaria',
        'raca_cor', 'raca_agregada', 'nivel_instrucao',
        'cod_ocupacao', 'grande_grupo',
        'grupamento_atividade', 'setor_agregado',
        'posicao_ocupacao', 'formal',
        'rendimento_habitual', 'rendimento_winsor', 'rendimento_todos',
        'horas_trabalhadas', 'peso',
        'exposure_score', 'exposure_gradient', 'match_level',
        'quintil_exposure', 'decil_exposure'
    ]
    
    df_final = df[[c for c in cols_output if c in df.columns]]
    
    # Salvar
    output_path = DATA_PROCESSED / "pnad_ilo_merged.csv"
    df_final.to_csv(output_path, index=False)
    logger.info(f"\nBase final salva em: {output_path}")
    logger.info(f"Tamanho: {df_final.memory_usage(deep=True).sum()/1e6:.1f} MB")
    
    return df_final

if __name__ == "__main__":
    df = merge_and_finalize()
    print("\n‚úì Merge conclu√≠do com sucesso!")
```

---

### 2.6 src/utils/weighted_stats.py

```python
"""
Fun√ß√µes utilit√°rias para estat√≠sticas ponderadas
"""

import numpy as np
import pandas as pd

def weighted_mean(values, weights):
    """Calcula m√©dia ponderada"""
    mask = ~(pd.isna(values) | pd.isna(weights))
    if mask.sum() == 0:
        return np.nan
    return np.average(values[mask], weights=weights[mask])

def weighted_std(values, weights):
    """Calcula desvio-padr√£o ponderado"""
    mask = ~(pd.isna(values) | pd.isna(weights))
    if mask.sum() == 0:
        return np.nan
    avg = np.average(values[mask], weights=weights[mask])
    variance = np.average((values[mask] - avg) ** 2, weights=weights[mask])
    return np.sqrt(variance)

def weighted_quantile(values, weights, quantile):
    """Calcula quantil ponderado"""
    mask = ~(pd.isna(values) | pd.isna(weights))
    if mask.sum() == 0:
        return np.nan
    
    sorted_idx = np.argsort(values[mask])
    sorted_values = values[mask].iloc[sorted_idx]
    sorted_weights = weights[mask].iloc[sorted_idx]
    
    cumsum = np.cumsum(sorted_weights)
    cutoff = quantile * cumsum.iloc[-1]
    
    return sorted_values.iloc[np.searchsorted(cumsum, cutoff)]

def gini_coefficient(values, weights):
    """Calcula coeficiente de Gini ponderado"""
    mask = ~(pd.isna(values) | pd.isna(weights))
    if mask.sum() < 2:
        return np.nan
    
    x = np.array(values[mask])
    w = np.array(weights[mask])
    
    sorted_idx = np.argsort(x)
    sorted_x = x[sorted_idx]
    sorted_w = w[sorted_idx]
    
    cumsum_w = np.cumsum(sorted_w)
    cumsum_wx = np.cumsum(sorted_w * sorted_x)
    
    total_w = cumsum_w[-1]
    total_wx = cumsum_wx[-1]
    
    # √Årea sob curva de Lorenz
    B = np.sum(cumsum_wx[:-1] * sorted_w[1:]) / (total_w * total_wx)
    
    return 1 - 2 * B

def weighted_stats_summary(values, weights):
    """Retorna dicion√°rio com estat√≠sticas resumidas"""
    return {
        'mean': weighted_mean(values, weights),
        'std': weighted_std(values, weights),
        'p25': weighted_quantile(values, weights, 0.25),
        'p50': weighted_quantile(values, weights, 0.50),
        'p75': weighted_quantile(values, weights, 0.75),
        'n': (~pd.isna(values)).sum(),
        'population': weights[~pd.isna(values)].sum()
    }
```

---

### 2.7 src/06_analysis_tables.py

```python
"""
Script 06: Gera√ß√£o das 5 tabelas principais
Entrada: data/processed/pnad_ilo_merged.csv
Sa√≠da: outputs/tables/*.csv e *.tex
"""

import logging
import pandas as pd
import numpy as np
import sys
sys.path.append('..')
from config.settings import *
from src.utils.weighted_stats import *

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(OUTPUTS_LOGS / '06_tables.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_data():
    """Carrega base consolidada"""
    df = pd.read_csv(DATA_PROCESSED / "pnad_ilo_merged.csv")
    logger.info(f"Dados carregados: {len(df):,} observa√ß√µes")
    return df

def table1_exposicao_grupos(df):
    """TABELA 1: Exposi√ß√£o por Grande Grupo Ocupacional"""
    
    logger.info("\n=== TABELA 1: Exposi√ß√£o por Grande Grupo ===")
    
    results = []
    for grupo in df['grande_grupo'].dropna().unique():
        mask = df['grande_grupo'] == grupo
        subset = df[mask]
        
        results.append({
            'Grande Grupo': grupo,
            'Exposi√ß√£o M√©dia': weighted_mean(subset['exposure_score'], subset['peso']),
            'Desvio-Padr√£o': weighted_std(subset['exposure_score'], subset['peso']),
            'Trabalhadores (milh√µes)': subset['peso'].sum() / 1e6,
            '% For√ßa de Trabalho': subset['peso'].sum() / df['peso'].sum() * 100
        })
    
    tabela1 = pd.DataFrame(results)
    tabela1 = tabela1.sort_values('Exposi√ß√£o M√©dia', ascending=False)
    tabela1 = tabela1.round(3)
    
    # Salvar CSV
    tabela1.to_csv(OUTPUTS_TABLES / "tabela1_exposicao_grupos.csv", index=False)
    
    # Salvar LaTeX
    latex = tabela1.to_latex(
        index=False,
        caption='Exposi√ß√£o √† IA Generativa por Grande Grupo Ocupacional - Brasil 3¬∫ Tri 2025',
        label='tab:exposicao_grupos',
        column_format='lrrrr'
    )
    with open(OUTPUTS_TABLES / "tabela1_exposicao_grupos.tex", 'w') as f:
        f.write(latex)
    
    logger.info(tabela1.to_string())
    return tabela1

def table2_perfil_quintis(df):
    """TABELA 2: Perfil Socioecon√¥mico por Quintil de Exposi√ß√£o"""
    
    logger.info("\n=== TABELA 2: Perfil por Quintil ===")
    
    results = []
    for quintil in df['quintil_exposure'].dropna().unique():
        mask = df['quintil_exposure'] == quintil
        subset = df[mask]
        
        results.append({
            'Quintil': quintil,
            'Rendimento M√©dio (R$)': weighted_mean(subset['rendimento_habitual'], subset['peso']),
            '% Formal': weighted_mean(subset['formal'], subset['peso']) * 100,
            '% Mulheres': weighted_mean(subset['sexo_texto'] == 'Mulher', subset['peso']) * 100,
            '% Negros': weighted_mean(subset['raca_agregada'] == 'Negra', subset['peso']) * 100,
            'Idade M√©dia': weighted_mean(subset['idade'], subset['peso']),
            'Pop. (milh√µes)': subset['peso'].sum() / 1e6
        })
    
    tabela2 = pd.DataFrame(results)
    tabela2 = tabela2.sort_values('Quintil')
    tabela2 = tabela2.round(2)
    
    # Salvar
    tabela2.to_csv(OUTPUTS_TABLES / "tabela2_perfil_quintis.csv", index=False)
    
    latex = tabela2.to_latex(
        index=False,
        caption='Perfil Socioecon√¥mico por Quintil de Exposi√ß√£o √† IA - Brasil 3¬∫ Tri 2025',
        label='tab:perfil_quintis'
    )
    with open(OUTPUTS_TABLES / "tabela2_perfil_quintis.tex", 'w') as f:
        f.write(latex)
    
    logger.info(tabela2.to_string())
    return tabela2

def table3_regiao_setor(df):
    """TABELA 3: Exposi√ß√£o por Regi√£o e Setor (matriz)"""
    
    logger.info("\n=== TABELA 3: Regi√£o x Setor ===")
    
    # Calcular m√©dias ponderadas por c√©lula
    def calc_weighted_mean(group):
        return weighted_mean(group['exposure_score'], group['peso'])
    
    tabela3 = df.groupby(['regiao', 'setor_agregado']).apply(calc_weighted_mean).unstack(fill_value=np.nan)
    tabela3 = tabela3.round(3)
    
    # Adicionar m√©dias marginais
    tabela3['M√©dia Regi√£o'] = df.groupby('regiao').apply(calc_weighted_mean)
    
    setor_means = df.groupby('setor_agregado').apply(calc_weighted_mean)
    tabela3.loc['M√©dia Setor'] = setor_means
    tabela3.loc['M√©dia Setor', 'M√©dia Regi√£o'] = weighted_mean(df['exposure_score'], df['peso'])
    
    # Salvar
    tabela3.to_csv(OUTPUTS_TABLES / "tabela3_regiao_setor.csv")
    
    latex = tabela3.to_latex(
        caption='Exposi√ß√£o M√©dia √† IA por Regi√£o e Setor - Brasil 3¬∫ Tri 2025',
        label='tab:regiao_setor'
    )
    with open(OUTPUTS_TABLES / "tabela3_regiao_setor.tex", 'w') as f:
        f.write(latex)
    
    logger.info(tabela3.to_string())
    return tabela3

def table4_desigualdade(df):
    """TABELA 4: Decomposi√ß√£o da Desigualdade de Exposi√ß√£o"""
    
    logger.info("\n=== TABELA 4: Desigualdade ===")
    
    mask = df['exposure_score'].notna()
    subset = df[mask]
    
    results = {
        'M√©trica': [],
        'Valor': []
    }
    
    # Gini total
    gini = gini_coefficient(subset['exposure_score'], subset['peso'])
    results['M√©trica'].append('Coeficiente de Gini')
    results['Valor'].append(gini)
    
    # Raz√£o P90/P10
    p90 = weighted_quantile(subset['exposure_score'], subset['peso'], 0.90)
    p10 = weighted_quantile(subset['exposure_score'], subset['peso'], 0.10)
    results['M√©trica'].append('Raz√£o P90/P10')
    results['Valor'].append(p90 / p10 if p10 > 0 else np.nan)
    
    # Raz√£o Q5/Q1
    q5_mean = weighted_mean(
        subset[subset['quintil_exposure'] == 'Q5 (Alta)']['exposure_score'],
        subset[subset['quintil_exposure'] == 'Q5 (Alta)']['peso']
    )
    q1_mean = weighted_mean(
        subset[subset['quintil_exposure'] == 'Q1 (Baixa)']['exposure_score'],
        subset[subset['quintil_exposure'] == 'Q1 (Baixa)']['peso']
    )
    results['M√©trica'].append('Raz√£o M√©dia Q5/Q1')
    results['Valor'].append(q5_mean / q1_mean if q1_mean > 0 else np.nan)
    
    # % em alta exposi√ß√£o (Gradient 4)
    alta_exp = subset[subset['exposure_gradient'] == 'Gradient 4 (Alta)']['peso'].sum()
    results['M√©trica'].append('% Alta Exposi√ß√£o (G4)')
    results['Valor'].append(alta_exp / subset['peso'].sum() * 100)
    
    tabela4 = pd.DataFrame(results)
    tabela4 = tabela4.round(4)
    
    # Salvar
    tabela4.to_csv(OUTPUTS_TABLES / "tabela4_desigualdade.csv", index=False)
    
    latex = tabela4.to_latex(
        index=False,
        caption='M√©tricas de Desigualdade na Exposi√ß√£o √† IA - Brasil 3¬∫ Tri 2025',
        label='tab:desigualdade'
    )
    with open(OUTPUTS_TABLES / "tabela4_desigualdade.tex", 'w') as f:
        f.write(latex)
    
    logger.info(tabela4.to_string())
    return tabela4

def table5_comparacao(df):
    """TABELA 5: Compara√ß√£o com Literatura"""
    
    logger.info("\n=== TABELA 5: Compara√ß√£o ===")
    
    mask = df['exposure_score'].notna()
    media_br = weighted_mean(df[mask]['exposure_score'], df[mask]['peso'])
    
    # Alta exposi√ß√£o (score >= 0.45)
    alta_exp = df[df['exposure_score'] >= 0.45]['peso'].sum() / df[mask]['peso'].sum() * 100
    
    tabela5 = pd.DataFrame({
        'Estudo': [
            'Presente estudo (ILO 2025)',
            'Imaizumi et al. (LCA, 2024)*',
            'Gmyrek et al. (ILO, 2023)*',
            'Eloundou et al. (OpenAI, 2023)*'
        ],
        'Metodologia': [
            'ILO refined index 2025',
            'ILO 2023 + PNAD',
            'ILO 2023 global',
            'GPT exposure rubrics'
        ],
        'Pa√≠s/Regi√£o': [
            'Brasil',
            'Brasil',
            'Global',
            'EUA'
        ],
        'Exposi√ß√£o M√©dia': [
            media_br,
            np.nan,  # Preencher manualmente ap√≥s consultar literatura
            0.30,    # Valor aproximado ILO 2023
            np.nan   # Diferente metodologia
        ],
        '% Alta Exposi√ß√£o': [
            alta_exp,
            np.nan,
            np.nan,
            np.nan
        ]
    })
    
    tabela5.to_csv(OUTPUTS_TABLES / "tabela5_comparacao.csv", index=False)
    
    logger.info("* Valores a serem preenchidos ap√≥s consulta √† literatura")
    logger.info(tabela5.to_string())
    return tabela5

def generate_all_tables():
    """Gera todas as tabelas"""
    
    logger.info("=" * 60)
    logger.info("GERA√á√ÉO DE TABELAS - ETAPA 1")
    logger.info("=" * 60)
    
    df = load_data()
    
    t1 = table1_exposicao_grupos(df)
    t2 = table2_perfil_quintis(df)
    t3 = table3_regiao_setor(df)
    t4 = table4_desigualdade(df)
    t5 = table5_comparacao(df)
    
    logger.info("\n" + "=" * 60)
    logger.info("TABELAS GERADAS COM SUCESSO!")
    logger.info(f"Arquivos salvos em: {OUTPUTS_TABLES}")
    logger.info("=" * 60)
    
    return {'t1': t1, 't2': t2, 't3': t3, 't4': t4, 't5': t5}

if __name__ == "__main__":
    generate_all_tables()
```

---

### 2.8 src/07_analysis_figures.py

```python
"""
Script 07: Gera√ß√£o das 4 figuras principais
Entrada: data/processed/pnad_ilo_merged.csv
Sa√≠da: outputs/figures/*.png e *.pdf
"""

import logging
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import linregress
import sys
sys.path.append('..')
from config.settings import *
from src.utils.weighted_stats import *

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(OUTPUTS_LOGS / '07_figures.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Configura√ß√µes globais de estilo
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 11
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12
sns.set_style("whitegrid")

def load_data():
    """Carrega base consolidada"""
    df = pd.read_csv(DATA_PROCESSED / "pnad_ilo_merged.csv")
    logger.info(f"Dados carregados: {len(df):,} observa√ß√µes")
    return df

def figure1_distribuicao(df):
    """FIGURA 1: Distribui√ß√£o da Exposi√ß√£o (dois pain√©is)"""
    
    logger.info("\n=== FIGURA 1: Distribui√ß√£o da Exposi√ß√£o ===")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    mask = df['exposure_score'].notna()
    subset = df[mask]
    
    # Painel A: Histograma ponderado
    ax1.hist(
        subset['exposure_score'], 
        weights=subset['peso'] / 1e6,  # Em milh√µes
        bins=50, 
        edgecolor='black', 
        alpha=0.7, 
        color='steelblue'
    )
    
    media = weighted_mean(subset['exposure_score'], subset['peso'])
    ax1.axvline(media, color='red', linestyle='--', linewidth=2, 
                label=f'M√©dia: {media:.3f}')
    
    ax1.set_xlabel('Score de Exposi√ß√£o √† IA Generativa')
    ax1.set_ylabel('Trabalhadores (milh√µes)')
    ax1.set_title('(A) Distribui√ß√£o Cont√≠nua da Exposi√ß√£o')
    ax1.legend()
    
    # Painel B: Barras por gradiente
    gradient_counts = subset.groupby('exposure_gradient')['peso'].sum() / 1e6
    gradient_order = ['Not Exposed', 'Minimal Exposure', 'Gradient 1-2', 'Gradient 3', 'Gradient 4 (Alta)']
    gradient_counts = gradient_counts.reindex([g for g in gradient_order if g in gradient_counts.index])
    
    colors = ['#2ca02c', '#98df8a', '#ffbb78', '#ff7f0e', '#d62728'][:len(gradient_counts)]
    ax2.barh(range(len(gradient_counts)), gradient_counts.values, 
             color=colors, edgecolor='black')
    ax2.set_yticks(range(len(gradient_counts)))
    ax2.set_yticklabels(gradient_counts.index)
    ax2.set_xlabel('Trabalhadores (milh√µes)')
    ax2.set_title('(B) Distribui√ß√£o por Gradiente ILO')
    
    # Adicionar valores nas barras
    for i, v in enumerate(gradient_counts.values):
        ax2.text(v + 0.5, i, f'{v:.1f}M', va='center')
    
    plt.suptitle('Distribui√ß√£o da Exposi√ß√£o √† IA Generativa na For√ßa de Trabalho Brasileira\n3¬∫ Trimestre 2025', 
                 fontsize=14, y=1.02)
    plt.tight_layout()
    
    # Salvar
    for ext in ['png', 'pdf']:
        fig.savefig(OUTPUTS_FIGURES / f'fig1_distribuicao_exposicao.{ext}', 
                   dpi=300, bbox_inches='tight')
    
    logger.info(f"Figura 1 salva em {OUTPUTS_FIGURES}")
    plt.close()

def figure2_heatmap(df):
    """FIGURA 2: Heatmap Regi√£o x Setor"""
    
    logger.info("\n=== FIGURA 2: Heatmap Regi√£o x Setor ===")
    
    def calc_weighted_mean(group):
        return weighted_mean(group['exposure_score'], group['peso'])
    
    pivot = df.groupby(['regiao', 'setor_agregado']).apply(calc_weighted_mean).unstack()
    
    fig, ax = plt.subplots(figsize=(12, 8))
    
    sns.heatmap(
        pivot, 
        annot=True, 
        fmt='.3f', 
        cmap='YlOrRd',
        cbar_kws={'label': 'Score de Exposi√ß√£o'},
        ax=ax,
        linewidths=0.5
    )
    
    ax.set_title('Exposi√ß√£o √† IA Generativa por Regi√£o e Setor\nBrasil - 3¬∫ Trimestre 2025', fontsize=14)
    ax.set_xlabel('Setor de Atividade')
    ax.set_ylabel('Regi√£o')
    
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    for ext in ['png', 'pdf']:
        fig.savefig(OUTPUTS_FIGURES / f'fig2_heatmap_regiao_setor.{ext}', 
                   dpi=300, bbox_inches='tight')
    
    logger.info(f"Figura 2 salva em {OUTPUTS_FIGURES}")
    plt.close()

def figure3_renda(df):
    """FIGURA 3: Perfil Salarial por Decil de Exposi√ß√£o"""
    
    logger.info("\n=== FIGURA 3: Renda por Exposi√ß√£o ===")
    
    mask = df['decil_exposure'].notna()
    subset = df[mask]
    
    renda_decil = subset.groupby('decil_exposure').apply(
        lambda x: weighted_mean(x['rendimento_habitual'], x['peso'])
    )
    
    fig, ax = plt.subplots(figsize=(10, 6))
    
    bars = ax.bar(range(len(renda_decil)), renda_decil.values, 
                  color='teal', edgecolor='black', alpha=0.8)
    
    ax.set_xticks(range(len(renda_decil)))
    ax.set_xticklabels(renda_decil.index, rotation=0)
    ax.set_xlabel('Decil de Exposi√ß√£o √† IA')
    ax.set_ylabel('Rendimento M√©dio Mensal (R$)')
    ax.set_title('Rendimento M√©dio por Decil de Exposi√ß√£o √† IA Generativa\nBrasil - 3¬∫ Trimestre 2025')
    
    # Linha de tend√™ncia
    x_numeric = np.arange(1, len(renda_decil) + 1)
    slope, intercept, r_value, p_value, std_err = linregress(x_numeric, renda_decil.values)
    
    trend_line = slope * x_numeric + intercept
    ax.plot(range(len(renda_decil)), trend_line, 'r--', linewidth=2,
            label=f'Tend√™ncia (R¬≤ = {r_value**2:.3f})')
    
    ax.legend()
    ax.grid(axis='y', alpha=0.3)
    
    # Formatar valores no eixo Y
    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'R$ {x:,.0f}'))
    
    plt.tight_layout()
    
    for ext in ['png', 'pdf']:
        fig.savefig(OUTPUTS_FIGURES / f'fig3_renda_exposicao.{ext}', 
                   dpi=300, bbox_inches='tight')
    
    logger.info(f"Figura 3 salva em {OUTPUTS_FIGURES}")
    logger.info(f"Correla√ß√£o exposi√ß√£o-renda: R¬≤ = {r_value**2:.3f}")
    plt.close()

def figure4_decomposicao(df):
    """FIGURA 4: Decomposi√ß√£o Demogr√°fica (4 pain√©is)"""
    
    logger.info("\n=== FIGURA 4: Decomposi√ß√£o Demogr√°fica ===")
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    mask = df['exposure_gradient'].notna()
    subset = df[mask]
    
    gradient_order = ['Not Exposed', 'Minimal Exposure', 'Gradient 1-2', 'Gradient 3', 'Gradient 4 (Alta)']
    
    # Painel A: Por g√™nero
    gender_comp = subset.groupby(['exposure_gradient', 'sexo_texto'])['peso'].sum().unstack(fill_value=0)
    gender_comp = gender_comp.reindex([g for g in gradient_order if g in gender_comp.index])
    gender_pct = gender_comp.div(gender_comp.sum(axis=1), axis=0)
    
    gender_pct.plot(kind='barh', stacked=True, ax=axes[0,0], 
                   color=['lightblue', 'lightpink'], edgecolor='black')
    axes[0,0].set_title('(A) Composi√ß√£o por G√™nero')
    axes[0,0].set_xlabel('Propor√ß√£o')
    axes[0,0].legend(title='Sexo')
    
    # Painel B: Por ra√ßa
    race_comp = subset.groupby(['exposure_gradient', 'raca_agregada'])['peso'].sum().unstack(fill_value=0)
    race_comp = race_comp.reindex([g for g in gradient_order if g in race_comp.index])
    race_pct = race_comp.div(race_comp.sum(axis=1), axis=0)
    
    race_pct.plot(kind='barh', stacked=True, ax=axes[0,1], 
                 color=['#d4a574', '#8b6914', '#a9a9a9'], edgecolor='black')
    axes[0,1].set_title('(B) Composi√ß√£o por Ra√ßa')
    axes[0,1].set_xlabel('Propor√ß√£o')
    axes[0,1].legend(title='Ra√ßa')
    
    # Painel C: Por formalidade
    formal_comp = subset.groupby(['exposure_gradient', 'formal'])['peso'].sum().unstack(fill_value=0)
    formal_comp = formal_comp.reindex([g for g in gradient_order if g in formal_comp.index])
    formal_comp.columns = ['Informal', 'Formal']
    formal_pct = formal_comp.div(formal_comp.sum(axis=1), axis=0)
    
    formal_pct.plot(kind='barh', stacked=True, ax=axes[1,0], 
                   color=['lightcoral', 'lightgreen'], edgecolor='black')
    axes[1,0].set_title('(C) Composi√ß√£o por Formalidade')
    axes[1,0].set_xlabel('Propor√ß√£o')
    axes[1,0].legend(title='V√≠nculo')
    
    # Painel D: Por faixa et√°ria
    age_comp = subset.groupby(['exposure_gradient', 'faixa_etaria'])['peso'].sum().unstack(fill_value=0)
    age_comp = age_comp.reindex([g for g in gradient_order if g in age_comp.index])
    age_pct = age_comp.div(age_comp.sum(axis=1), axis=0)
    
    cmap = plt.cm.viridis(np.linspace(0.2, 0.8, len(age_pct.columns)))
    age_pct.plot(kind='barh', stacked=True, ax=axes[1,1], 
                color=cmap, edgecolor='black')
    axes[1,1].set_title('(D) Composi√ß√£o por Faixa Et√°ria')
    axes[1,1].set_xlabel('Propor√ß√£o')
    axes[1,1].legend(title='Idade', bbox_to_anchor=(1.05, 1))
    
    plt.suptitle('Composi√ß√£o Demogr√°fica por Gradiente de Exposi√ß√£o √† IA\nBrasil - 3¬∫ Trimestre 2025', 
                 fontsize=14, y=1.02)
    plt.tight_layout()
    
    for ext in ['png', 'pdf']:
        fig.savefig(OUTPUTS_FIGURES / f'fig4_decomposicao_demografica.{ext}', 
                   dpi=300, bbox_inches='tight')
    
    logger.info(f"Figura 4 salva em {OUTPUTS_FIGURES}")
    plt.close()

def generate_all_figures():
    """Gera todas as figuras"""
    
    logger.info("=" * 60)
    logger.info("GERA√á√ÉO DE FIGURAS - ETAPA 1")
    logger.info("=" * 60)
    
    df = load_data()
    
    figure1_distribuicao(df)
    figure2_heatmap(df)
    figure3_renda(df)
    figure4_decomposicao(df)
    
    logger.info("\n" + "=" * 60)
    logger.info("FIGURAS GERADAS COM SUCESSO!")
    logger.info(f"Arquivos salvos em: {OUTPUTS_FIGURES}")
    logger.info("=" * 60)

if __name__ == "__main__":
    generate_all_figures()
```

---

## PARTE 3: TESTES INTERMEDI√ÅRIOS

### 3.1 tests/test_01_download.py

```python
"""
Teste: Validar download PNAD
Executar ap√≥s: 01_download_pnad.py
"""

import pandas as pd
import sys
sys.path.append('..')
from config.settings import *

def test_download():
    """Testa se download foi bem sucedido"""
    
    file_path = DATA_RAW / f"pnad_{PNAD_ANO}q{PNAD_TRIMESTRE}.parquet"
    
    # Teste 1: Arquivo existe
    assert file_path.exists(), f"Arquivo n√£o encontrado: {file_path}"
    print("‚úì Arquivo existe")
    
    # Teste 2: Carregar e verificar tamanho
    df = pd.read_parquet(file_path)
    assert len(df) > 100000, f"Dados insuficientes: {len(df)} linhas"
    print(f"‚úì {len(df):,} observa√ß√µes carregadas")
    
    # Teste 3: Colunas esperadas
    expected_cols = ['cod_ocupacao', 'peso', 'rendimento_habitual', 'idade', 'sigla_uf']
    for col in expected_cols:
        assert col in df.columns, f"Coluna ausente: {col}"
    print(f"‚úì Colunas esperadas presentes")
    
    # Teste 4: Todas UFs presentes
    ufs = df['sigla_uf'].nunique()
    assert ufs == 27, f"UFs incompletas: {ufs}/27"
    print(f"‚úì Todas 27 UFs presentes")
    
    # Teste 5: Peso v√°lido
    assert df['peso'].sum() > 50e6, "Peso total muito baixo"
    print(f"‚úì Popula√ß√£o: {df['peso'].sum()/1e6:.1f} milh√µes")
    
    print("\nüéâ TODOS OS TESTES PASSARAM - DOWNLOAD OK!")
    return True

if __name__ == "__main__":
    test_download()
```

### 3.2 tests/test_04_crosswalk.py

```python
"""
Teste: Validar crosswalk COD-ISCO
Executar ap√≥s: 04_crosswalk.py
"""

import pandas as pd
import numpy as np
import sys
sys.path.append('..')
from config.settings import *

def test_crosswalk():
    """Testa qualidade do crosswalk"""
    
    df = pd.read_csv(DATA_PROCESSED / "pnad_clean.csv")
    
    # Simular crosswalk ou carregar resultado
    # (ajustar conforme implementa√ß√£o)
    
    # Teste 1: Cobertura m√≠nima 90%
    # coverage = df['exposure_score'].notna().mean()
    # assert coverage >= 0.90, f"Cobertura baixa: {coverage:.1%}"
    
    # Teste 2: Sanity check - grupos manuais devem ter exposi√ß√£o esperada
    # Ex: Apoio administrativo > Agropecu√°ria
    
    print("‚úì Testes de crosswalk definidos")
    print("‚ö† Executar ap√≥s script 04_crosswalk.py")
    
    return True

if __name__ == "__main__":
    test_crosswalk()
```

### 3.3 tests/test_05_merge.py

```python
"""
Teste: Validar base final merged
Executar ap√≥s: 05_merge_data.py
"""

import pandas as pd
import numpy as np
import sys
sys.path.append('..')
from config.settings import *

def test_merged_data():
    """Testa integridade da base final"""
    
    file_path = DATA_PROCESSED / "pnad_ilo_merged.csv"
    
    # Teste 1: Arquivo existe
    assert file_path.exists(), f"Arquivo n√£o encontrado: {file_path}"
    print("‚úì Arquivo existe")
    
    df = pd.read_csv(file_path)
    
    # Teste 2: Colunas essenciais
    required = [
        'exposure_score', 'exposure_gradient', 'quintil_exposure',
        'grande_grupo', 'regiao', 'setor_agregado', 'peso',
        'rendimento_habitual', 'sexo_texto', 'raca_agregada', 'formal'
    ]
    for col in required:
        assert col in df.columns, f"Coluna ausente: {col}"
    print("‚úì Colunas essenciais presentes")
    
    # Teste 3: Cobertura de exposure_score
    coverage = df['exposure_score'].notna().mean()
    assert coverage >= 0.85, f"Cobertura de score baixa: {coverage:.1%}"
    print(f"‚úì Cobertura de scores: {coverage:.1%}")
    
    # Teste 4: Score no range v√°lido [0, 1]
    scores = df['exposure_score'].dropna()
    assert scores.min() >= 0, f"Score m√≠nimo inv√°lido: {scores.min()}"
    assert scores.max() <= 1, f"Score m√°ximo inv√°lido: {scores.max()}"
    print(f"‚úì Scores no range [0, 1]: [{scores.min():.3f}, {scores.max():.3f}]")
    
    # Teste 5: Gradientes atribu√≠dos
    grad_coverage = df['exposure_gradient'].notna().mean()
    assert grad_coverage >= 0.85, f"Gradientes incompletos: {grad_coverage:.1%}"
    print(f"‚úì Gradientes atribu√≠dos: {grad_coverage:.1%}")
    
    # Teste 6: Quintis balanceados
    quintil_counts = df['quintil_exposure'].value_counts()
    min_pct = quintil_counts.min() / len(df) * 100
    max_pct = quintil_counts.max() / len(df) * 100
    assert max_pct - min_pct < 10, "Quintis muito desbalanceados"
    print(f"‚úì Quintis balanceados ({min_pct:.1f}% - {max_pct:.1f}%)")
    
    # Teste 7: Peso total consistente
    pop = df['peso'].sum() / 1e6
    assert 50 < pop < 150, f"Popula√ß√£o fora do esperado: {pop:.1f}M"
    print(f"‚úì Popula√ß√£o: {pop:.1f} milh√µes")
    
    print("\nüéâ TODOS OS TESTES PASSARAM - BASE FINAL OK!")
    return True

if __name__ == "__main__":
    test_merged_data()
```

---

## PARTE 4: SCRIPT MASTER

### run_all.py

```python
"""
Script Master: Executa todo o pipeline da Etapa 1
"""

import subprocess
import sys
from pathlib import Path
from datetime import datetime

def run_script(script_name):
    """Executa um script e retorna status"""
    print(f"\n{'='*60}")
    print(f"EXECUTANDO: {script_name}")
    print(f"{'='*60}\n")
    
    result = subprocess.run(
        [sys.executable, f"src/{script_name}"],
        capture_output=True,
        text=True
    )
    
    if result.returncode != 0:
        print(f"‚ùå ERRO em {script_name}:")
        print(result.stderr)
        return False
    
    print(result.stdout)
    print(f"‚úì {script_name} conclu√≠do com sucesso")
    return True

def run_test(test_name):
    """Executa um teste"""
    print(f"\n>>> Executando teste: {test_name}")
    result = subprocess.run(
        [sys.executable, f"tests/{test_name}"],
        capture_output=True,
        text=True
    )
    print(result.stdout)
    return result.returncode == 0

def main():
    """Pipeline completo"""
    
    start_time = datetime.now()
    print("=" * 60)
    print("ETAPA 1 - PIPELINE COMPLETO")
    print(f"In√≠cio: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # Etapa 1: Download PNAD
    if not run_script("01_download_pnad.py"):
        return False
    run_test("test_01_download.py")
    
    # Etapa 2: Processar ILO
    if not run_script("02_process_ilo.py"):
        return False
    
    # Etapa 3: Limpar PNAD
    if not run_script("03_clean_pnad.py"):
        return False
    
    # Etapa 4: Crosswalk (valida√ß√£o manual recomendada)
    print("\n‚ö† ATEN√á√ÉO: Revise o log de crosswalk antes de prosseguir")
    input("Pressione ENTER para continuar...")
    
    # Etapa 5: Merge
    if not run_script("05_merge_data.py"):
        return False
    run_test("test_05_merge.py")
    
    # Etapa 6: Gerar Tabelas
    if not run_script("06_analysis_tables.py"):
        return False
    
    # Etapa 7: Gerar Figuras
    if not run_script("07_analysis_figures.py"):
        return False
    
    # Finaliza√ß√£o
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds() / 60
    
    print("\n" + "=" * 60)
    print("üéâ PIPELINE CONCLU√çDO COM SUCESSO!")
    print(f"Dura√ß√£o total: {duration:.1f} minutos")
    print(f"Fim: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    print("\nüìÅ OUTPUTS GERADOS:")
    print("  - data/processed/pnad_ilo_merged.csv")
    print("  - outputs/tables/*.csv e *.tex")
    print("  - outputs/figures/*.png e *.pdf")
    print("  - outputs/logs/*.log")
    
    return True

if __name__ == "__main__":
    main()
```

---

## PARTE 5: REQUIREMENTS.TXT

```
# Core
pandas>=2.0.0
numpy>=1.24.0
pyarrow>=12.0.0

# BigQuery / Google Cloud
basedosdados>=1.6.0
google-cloud-bigquery>=3.0.0
google-auth>=2.0.0

# Visualiza√ß√£o
matplotlib>=3.7.0
seaborn>=0.12.0

# Estat√≠stica
scipy>=1.10.0
statsmodels>=0.14.0

# Excel
openpyxl>=3.1.0
xlrd>=2.0.0

# Utilit√°rios
requests>=2.28.0
tqdm>=4.65.0

# Opcional - NLP para crosswalk avan√ßado
# sentence-transformers>=2.2.0
```

---

## CHECKLIST DE EXECU√á√ÉO

### Antes de come√ßar
- [ ] Criar projeto no Google Cloud Console
- [ ] Configurar billing (free tier √© suficiente)
- [ ] Instalar Google Cloud SDK e autenticar
- [ ] Baixar arquivo ILO do GitHub
- [ ] Instalar depend√™ncias: `pip install -r requirements.txt`

### Execu√ß√£o sequencial
- [ ] 01: Baixar PNAD ‚Üí Testar com `test_01_download.py`
- [ ] 02: Processar ILO ‚Üí Verificar log `02_ilo_process.log`
- [ ] 03: Limpar PNAD ‚Üí Verificar estat√≠sticas no log
- [ ] 04: Crosswalk ‚Üí **REVISAR LOG CUIDADOSAMENTE** (ponto cr√≠tico)
- [ ] 05: Merge ‚Üí Testar com `test_05_merge.py`
- [ ] 06: Tabelas ‚Üí Verificar `outputs/tables/`
- [ ] 07: Figuras ‚Üí Verificar `outputs/figures/`

### Ap√≥s conclus√£o
- [ ] Revisar todos os logs em `outputs/logs/`
- [ ] Verificar se tabelas est√£o coerentes
- [ ] Verificar se figuras est√£o leg√≠veis
- [ ] Preencher RESULTADOS_ETAPA1.md

---

## PONTOS DE ATEN√á√ÉO

1. **Crosswalk √© o ponto cr√≠tico** - Taxa de match 4-digit deve ser >60%, total >95%
2. **Sempre usar pesos** - Nunca calcular m√©dia/contagem sem ponderar
3. **Validar sanidade** - Grupo 4 (administrativo) deve ter exposi√ß√£o alta, Grupo 9 (elementares) baixa
4. **Documentar decis√µes** - Anotar qualquer ajuste feito nos scripts

---

*Arquivo criado em: Janeiro 2026*
*Para uso com Cursor AI*
